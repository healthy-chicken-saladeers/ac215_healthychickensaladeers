{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01ff872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import ChromiumOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0d4c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cffc56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chrome_options() -> ChromiumOptions:\n",
    "    \"\"\"Sets chrome options for Selenium.Chrome options for headless browser is enabled.\n",
    "    Args: None\n",
    "    \n",
    "    returns:\n",
    "        Chrome options that can work headless i.e. without actually launching the browser.\n",
    "    \"\"\"\n",
    "    chrome_options = ChromiumOptions()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_prefs = {}\n",
    "    chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "    chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "    return chrome_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e9c37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sitemap(url):\n",
    "    \"\"\"\n",
    "    Extracts all the links from a company's sitemap.xml.\n",
    "    \n",
    "    Args:\n",
    "    url (str): The URL for a company sitemap.xml.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A pandas Series with all the links, or None if no links found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with requests.get(url, headers=headers) as response:\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "            soup = BeautifulSoup(response.text, 'lxml-xml')\n",
    "            urls = [link.text.strip() for link in soup.find_all('loc') if link]\n",
    "            \n",
    "            if not urls:\n",
    "                return None  # Return None if no URLs found\n",
    "            \n",
    "            extended_urls = []\n",
    "            for link in urls:\n",
    "                if link.endswith('xml'):\n",
    "                    try:\n",
    "                        with requests.get(link, headers=headers) as response:\n",
    "                            response.raise_for_status()  # Check if the request was successful\n",
    "                            nested_soup = BeautifulSoup(response.text, 'lxml')\n",
    "                            nested_urls = [url.text.strip() for url in nested_soup.find_all('loc') if url]\n",
    "                            extended_urls.extend(nested_urls)\n",
    "                    except requests.RequestException as e:\n",
    "                        print(f\"Error occurred while processing {link}: {e}\")\n",
    "                else:\n",
    "                    extended_urls.append(link)\n",
    "            \n",
    "            if not extended_urls:\n",
    "                return None  # Return None if no extended URLs found\n",
    "            \n",
    "            return pd.Series(extended_urls).drop_duplicates().str.strip()\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None  # Return None if the initial request fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fe28c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_website(all_links, options):\n",
    "    \"\"\"\n",
    "    Extracts all the text data from the webpages of a company.\n",
    "\n",
    "    Args:\n",
    "    all_links (pd.Series): A pandas Series with all the links in the company's website.\n",
    "    options: Chrome options to apply to the browser\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame with the columns 'key' (webpage link), 'text' (includes \n",
    "                  the text of the webpage), and 'timestamp' (when the data was scraped).\n",
    "    \n",
    "    pd.DataFrame : A pandas dataframe with columns 'key' (webpage link), 'error with timestamp' .\n",
    "    \"\"\"\n",
    "    log_dict = {}\n",
    "    text_dict = {}\n",
    "    wait_condition = (By.TAG_NAME,['html','div','body'])\n",
    "\n",
    "    for link in all_links.to_list():\n",
    "        try:\n",
    "            # First, scrape the page using requests\n",
    "            with requests.get(link, headers=headers) as response:\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    \n",
    "                    [tag.decompose() for tag in soup.find_all(['header', 'nav', 'footer'])]\n",
    "                    text_only_requests = soup.get_text(separator=' ', strip=True)\n",
    "                    print(link)\n",
    "                \n",
    "                # If content seems too short or response code is not 200, use Selenium\n",
    "                if response.status_code != 200 or len(text_only_requests.split()) < 50:\n",
    "                    try:\n",
    "                        browser = webdriver.Chrome(options)\n",
    "                        browser.get(link)\n",
    "                        wait = WebDriverWait(browser, timeout=30)\n",
    "\n",
    "                        wait.until(lambda d: browser.find_element(By.TAG_NAME, 'html').is_displayed() \\\n",
    "                                   if browser.find_element(By.TAG_NAME, 'html') else True &\\\n",
    "                                   \n",
    "                                  np.all(np.array([i.is_displayed() \\\n",
    "                                                       for i in browser.find_elements(By.TAG_NAME, 'div')])) \\\n",
    "                                   \n",
    "                                   if browser.find_elements(By.TAG_NAME, 'div') else True)\n",
    "                                   \n",
    "                               \n",
    "                                  \n",
    "                        soup_selenium = BeautifulSoup(browser.page_source, 'lxml')\n",
    "\n",
    "                        [tag.decompose() for tag in soup_selenium.find_all(['header', 'nav', 'footer'])]\n",
    "                        text_only_selenium = soup_selenium.get_text(separator=' ', strip=True).lower()\n",
    "\n",
    "                        text_dict[link] = text_only_selenium\n",
    "                        \n",
    "                        if len(text_only_selenium.lower().split()) < 20:\n",
    "                            log_dict[link] = str(pd.to_datetime(datetime.today().date())) + \\\n",
    "                            \" \"+text_only_selenium\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred while processing {link} in selenium: {e.with_traceback}\")\n",
    "                        log_dict[link] = f'{pd.to_datetime(datetime.today().date())}  {e.with_traceback}'\n",
    "                       \n",
    "                    finally:\n",
    "                        browser.close()\n",
    "                                \n",
    "                else:\n",
    "                    text_dict[link] = text_only_requests.lower()\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error occurred while processing {link}: {e}\")\n",
    "            log_dict[link] = f'pd.to_datetime(datetime.today().date())  {e.with_traceback}'\n",
    "    if not log_dict:\n",
    "        df_log = pd.DataFrame()\n",
    "    else:\n",
    "        \n",
    "        df_log = pd.DataFrame(list(log_dict.items()), columns=['key', 'error'])\n",
    "    \n",
    "    if not text_dict:\n",
    "        return pd.DataFrame(), df_log   # return empty DataFrame if no text is extracted\n",
    "\n",
    "    df = pd.DataFrame(list(text_dict.items()), columns=['key', 'text'])\n",
    "    df['timestamp'] = pd.to_datetime(datetime.today().date())\n",
    "    \n",
    "\n",
    "    return df, df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59c79437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Runs the scraping engine, extracts data from the specified sitemaps, and saves\n",
    "    the extracted data to a CSV file.\n",
    "    \n",
    "    The CSV file contains the scraped data from the first 10 webpages of each sitemap.\n",
    "    \"\"\"\n",
    "    options = set_chrome_options()\n",
    "    try:\n",
    "        sitemap_df = pd.read_csv(\"sitemap.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file 'sitemap.csv' was not found.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file 'sitemap.csv' is empty.\")\n",
    "        return\n",
    "\n",
    "    if 'sitemap' not in sitemap_df.columns:\n",
    "        print(\"Error: The expected 'sitemap' column is missing in 'sitemap.csv'.\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting scraping:\\n\")\n",
    "    for index, item in enumerate(sitemap_df['sitemap'], start=1):\n",
    "        print(f\"Working on {item} ...\")\n",
    "        links = scrape_sitemap(item)\n",
    "        if links is None or links.empty:\n",
    "            print(f\"No links were found in {item}.\\n\")\n",
    "            continue\n",
    "\n",
    "        link_split = item.split('/')\n",
    "        if link_split:\n",
    "            website_name = link_split[2]\n",
    "        print(f\"{links.shape[0]} webpages found for scraping. For demo, scraping only the first 10 webpages.\\n\")\n",
    "        \n",
    "        scraped_df, log_df = scrape_website(links[:3], options)\n",
    "        if scraped_df.empty:\n",
    "            print(f\"No data was scraped from the first 10 links of {item}.\\n\")\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        timestamp = str(datetime.now()).split()[0]    \n",
    "        output_file = f'data/{website_name}_{timestamp}.csv'\n",
    "        scraped_df.to_csv(output_file, index=False)\n",
    "        if not log_df.empty:\n",
    "            log_file = f'log/{website_name}_{timestamp}.csv'\n",
    "            log_df.to_csv(log_file, index=False)\n",
    "        \n",
    "        \n",
    "        print(f\"Finished scraping. Data stored in {output_file}\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c56cb89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping:\n",
      "\n",
      "Working on https://cohere.com/sitemap.xml ...\n",
      "30 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://cohere.com/responsibility\n",
      "https://cohere.com/deployment-options/oracle\n",
      "https://cohere.com/news\n",
      "Finished scraping. Data stored in data/cohere.com_2023-10-04.csv\n",
      "\n",
      "Working on https://ai21.com/sitemap.xml ...\n",
      "91 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.ai21.com/\n",
      "https://www.ai21.com/about\n",
      "https://www.ai21.com/ai-co-writing\n",
      "Finished scraping. Data stored in data/ai21.com_2023-10-04.csv\n",
      "\n",
      "Working on https://descript.com/sitemap.xml ...\n",
      "187 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.descript.com/about\n",
      "https://www.descript.com/affiliate\n",
      "https://www.descript.com/affiliate-terms\n",
      "Finished scraping. Data stored in data/descript.com_2023-10-04.csv\n",
      "\n",
      "Working on https://weaviate.io/sitemap.xml ...\n",
      "335 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://weaviate.io/blog\n",
      "https://weaviate.io/blog/ann-algorithms-hnsw-pq\n",
      "https://weaviate.io/blog/ann-algorithms-tiles-enocoder\n",
      "Finished scraping. Data stored in data/weaviate.io_2023-10-04.csv\n",
      "\n",
      "Working on https://assemblyai.com/sitemap.xml ...\n",
      "17 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.assemblyai.com/\n",
      "https://www.assemblyai.com/models/core-transcription\n",
      "https://www.assemblyai.com/models/audio-intelligence\n",
      "Finished scraping. Data stored in data/assemblyai.com_2023-10-04.csv\n",
      "\n",
      "Working on https://anthropic.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.anthropic.com/amazon-bedrock\n",
      "https://www.anthropic.com/claude-in-slack\n",
      "https://www.anthropic.com/claude-in-slack/support\n",
      "Finished scraping. Data stored in data/anthropic.com_2023-10-04.csv\n",
      "\n",
      "Working on https://inflection.ai/sitemap.xml ...\n",
      "15 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://inflection.ai\n",
      "https://inflection.ai/about\n",
      "https://inflection.ai/safety\n",
      "Finished scraping. Data stored in data/inflection.ai_2023-10-04.csv\n",
      "\n",
      "Working on https://h2o.ai/sitemap.xml ) ...\n",
      "983 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://h2o.ai/\n",
      "https://h2o.ai/br/\n",
      "https://h2o.ai/br/agradecemos-seu-contato/\n",
      "Finished scraping. Data stored in data/h2o.ai_2023-10-04.csv\n",
      "\n",
      "Working on https://harver.com/sitemap_index.xml  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://harver.com/blog/\n",
      "https://harver.com/blog/hr-tech-5-skills-modern-day-hr-professionals-need/\n",
      "https://harver.com/blog/hr-tech-5-recruitment-hacks/\n",
      "Finished scraping. Data stored in data/harver.com_2023-10-04.csv\n",
      "\n",
      "Working on https://dataminr.com/sitemap.xml ...\n",
      "451 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.dataminr.com/blog/dataminr-celebrates-pride-month\n",
      "https://www.dataminr.com/resources/storms-sweep-the-southeast\n",
      "https://www.dataminr.com/press/embracing-broad-data-sets-protecting-brand-reputation-at-the-world-cup\n",
      "Finished scraping. Data stored in data/dataminr.com_2023-10-04.csv\n",
      "\n",
      "Working on https://shield.ai/sitemap_index.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://shield.ai/blog/\n",
      "https://shield.ai/america-must-accelerate-scale-and-innovate-win-future-wars/\n",
      "https://shield.ai/on-overcoming-obstacles/\n",
      "Finished scraping. Data stored in data/shield.ai_2023-10-04.csv\n",
      "\n",
      "Working on https://kymeratx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.kymeratx.com/\n",
      "https://www.kymeratx.com/terms/\n",
      "https://www.kymeratx.com/privacy-policy/\n",
      "Finished scraping. Data stored in data/kymeratx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://arvinas.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.arvinas.com/\n",
      "https://www.arvinas.com/terms-of-use/\n",
      "https://www.arvinas.com/patients/\n",
      "Finished scraping. Data stored in data/arvinas.com_2023-10-04.csv\n",
      "\n",
      "Working on https://ardelyx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://ardelyx.com/\n",
      "https://ardelyx.com/our-pipeline/\n",
      "https://ardelyx.com/products/\n",
      "Finished scraping. Data stored in data/ardelyx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://monterosatx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.monterosatx.com/news/monte-rosa-therapeutics-strengthens-leadership-with-key-executive-and-board-appointments/\n",
      "https://www.monterosatx.com/news/monte-rosa-therapeutics-to-participate-in-upcoming-investor-conferences/\n",
      "https://www.monterosatx.com/news/monte-rosa-therapeutics-expands-senior-management-team/\n",
      "Finished scraping. Data stored in data/monterosatx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://trianabio.com/sitemap.xml ...\n",
      "31 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://trianabio.com/press-release-4-6-2022\n",
      "https://trianabio.com/job-posting-senior-scientist-lead-discovery\n",
      "https://images.squarespace-cdn.com/content/v1/61d5e7afb1ff8b1bd358de61/5855e10d-77f0-42cc-aa0f-5d4792c9244a/TRIANA_ID_Logomark_72.png\n",
      "Finished scraping. Data stored in data/trianabio.com_2023-10-04.csv\n",
      "\n",
      "Working on https://tangotx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.tangotx.com/\n",
      "https://www.tangotx.com/science/our-approach/\n",
      "https://www.tangotx.com/pipeline/\n",
      "Finished scraping. Data stored in data/tangotx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://vertex.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.vertex.com/our-blog-news-updates-trends-vertex-software/\n",
      "https://www.vertex.com/american-companies-saving-10m-annually-process-automation/\n",
      "https://www.vertex.com/hr-management-in-the-age-of-technology/\n",
      "Finished scraping. Data stored in data/vertex.com_2023-10-04.csv\n",
      "\n",
      "Working on https://vervetx.com/sitemap.xml ...\n",
      "28 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.vervetx.com/about-us/our-story\n",
      "https://www.vervetx.com/about-us/why-cardiovascular-disease\n",
      "https://www.vervetx.com/careers\n",
      "Finished scraping. Data stored in data/vervetx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://novonordisk.com/sitemap.xml ...\n",
      "319 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://www.novonordisk.com/\n",
      "https://www.novonordisk.com/disease-areas.html\n",
      "https://www.novonordisk.com/disease-areas/type-1-diabetes.html\n",
      "Finished scraping. Data stored in data/novonordisk.com_2023-10-04.csv\n",
      "\n",
      "Working on https://shionogi.com/us/en/sitemap.xml ...\n",
      "113 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "http://www.shionogi.com/us/en/\n",
      "http://www.shionogi.com/us/en/about.html\n",
      "http://www.shionogi.com/us/en/about/philosophy.html\n",
      "Finished scraping. Data stored in data/shionogi.com_2023-10-04.csv\n",
      "\n",
      "Working on https://alexion.com/sitemap.xml ...\n",
      "Error occurred: 404 Client Error: Not Found for url: https://alexion.com/sitemap.xml\n",
      "No links were found in https://alexion.com/sitemap.xml.\n",
      "\n",
      "Working on https://relaytx.com/sitemap.xml  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://relaytx.com/\n",
      "https://relaytx.com/terms-of-use/\n",
      "https://relaytx.com/privacy-policy/\n",
      "Finished scraping. Data stored in data/relaytx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://neumoratx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://neumoratx.com/?page_id=30\n",
      "https://neumoratx.com/news/amgen-and-neumora-therapeutics-announce-strategic-rd-collaboration-to-accelerate-novel-precision-therapies-for-brain-diseases/\n",
      "https://neumoratx.com/news/neumora-therapeutics-launches-to-pioneer-a-new-era-of-precision-medicines-for-brain-diseases/\n",
      "Finished scraping. Data stored in data/neumoratx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://verily.com/sitemap.xml ...\n",
      "210 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://verily.com/about-us\n",
      "https://verily.com/about-us/careers\n",
      "https://verily.com/about-us/careers/open-roles\n",
      "Finished scraping. Data stored in data/verily.com_2023-10-04.csv\n",
      "\n",
      "Working on https://kojintx.com/sitemap.xml ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riku/miniconda3/envs/cs115/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "https://kojintx.com/\n",
      "https://kojintx.com/sample-page/\n",
      "https://kojintx.com/privacy-policy/\n",
      "Finished scraping. Data stored in data/kojintx.com_2023-10-04.csv\n",
      "\n",
      "Working on https://bland.ai/sitemap.xml ...\n",
      "12 webpages found for scraping. For demo, scraping only the first 10 webpages.\n",
      "\n",
      "http://www.bland.ai/\n",
      "http://www.bland.ai/blog\n",
      "https://www.bland.ai/blog/enterprise-ai-phone-call-use-cases\n",
      "Finished scraping. Data stored in data/bland.ai_2023-10-04.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0628087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361f026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980fc38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107f81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814b5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e4ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
