{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934520c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"TextChunk\",\n",
    "            \"description\": \"A segmented portion of text from a scraped webpage with full details.\",\n",
    "            \"invertedIndexConfig\": {\n",
    "                \"indexTimestamps\": True\n",
    "            },\n",
    "            \"vectorizer\": \"text2vec-openai\",\n",
    "            \"moduleConfig\": {\n",
    "                \"generative-openai\": {\n",
    "                    \"model\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "            },\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"key\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The identifier for the text chunk.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"text\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The content of the text chunk.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"pageURL\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The specific URL of the scraped webpage this chunk belongs to.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"websiteAddress\",\n",
    "                    \"dataType\": [\"string\"],\n",
    "                    \"description\": \"The address of the website this chunk comes from.\",\n",
    "                    \"indexInverted\": True\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"timestamp\",\n",
    "                    \"dataType\": [\"date\"],\n",
    "                    \"description\": \"The date and time when the chunk was scraped.\",\n",
    "                    \"indexInverted\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fcac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema was created.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import csv\n",
    "from os import listdir\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def create_date(date_string):\n",
    "    # convert it to datetime object\n",
    "    dt_object = datetime.strptime(date_string, \"%Y-%m-%dT%H-%M-%S\")\n",
    "    # convert datetime object to RFC 3339 string (with timezone)\n",
    "    rfc3339_string = dt_object.replace(tzinfo=timezone.utc).isoformat()\n",
    "    return rfc3339_string\n",
    "\n",
    "\n",
    "# Initialize Weaviate client\n",
    "client = weaviate.Client(\n",
    "    # Weaviate instance URL\n",
    "    url=\"http://34.66.77.236:8080\",\n",
    "    additional_headers={\n",
    "        # Replace with your OpenAI key\n",
    "        \"X-OPENAI-Api-Key\": \"sk-OPEN_AI_KEY\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Delete existing schema (caution: this deletes the current structure)\n",
    "client.schema.delete_all()\n",
    "\n",
    "# Here we use the schema created in the previous cell.\n",
    "client.schema.create(schema)\n",
    "print(\"Schema was created.\")\n",
    "\n",
    "# Function to load data from CSV and extract website name and timestamp from filename\n",
    "def load_csv_data(directory):\n",
    "    all_data = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            website_name, timestamp = filename.rsplit('.', 1)[0].split('_')\n",
    "            timestamp = create_date(timestamp)\n",
    "            with open(f\"{directory}/{filename}\", mode='r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    row['websiteAddress'] = website_name\n",
    "                    row['timestamp'] = timestamp\n",
    "                    all_data.append(row)\n",
    "    return all_data\n",
    "\n",
    "# Load CSV data. TODO: Scraper needs to be modified to save CSVs here\n",
    "data_directory = '../data'\n",
    "\n",
    "csv_data = load_csv_data(data_directory)\n",
    "\n",
    "text_chunk_size = 500\n",
    "\n",
    "def split_into_chunks(string, text_chunk_size):\n",
    "    words = string.split()\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    \n",
    "    # Split into chunks\n",
    "    for idx, word in enumerate(words):\n",
    "        if idx % text_chunk_size == 0 and idx > 0:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = \"\"\n",
    "        chunk += word + \" \"\n",
    "    \n",
    "    # Add last chunk to list by concatenating with last chunk in the list\n",
    "    if chunk and len(chunks) > 0:\n",
    "        chunks[-1] += chunk.strip()\n",
    "        # chunks.append(chunk.strip())\n",
    "        \n",
    "    # If there's only one chunk, return list with one chunk\n",
    "    elif chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb781fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da37089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Get': {'TextChunk': []}}}\n"
     ]
    }
   ],
   "source": [
    "from weaviate.batch import Batch \n",
    "from weaviate.util import generate_uuid5 \n",
    "\n",
    "csv_website_address = csv_data[0]['websiteAddress']\n",
    "csv_timestamp = csv_data[0]['timestamp']\n",
    "\n",
    "def text_chunk_exists(client, website_address, timestamp):\n",
    "    query = f\"\"\"\n",
    "    {{\n",
    "      Get {{\n",
    "        TextChunk (where: {{\n",
    "            operator: And\n",
    "            operands: [{{\n",
    "                path: [\"websiteAddress\"],\n",
    "                operator: Equal,\n",
    "                valueString: \"{website_address}\"\n",
    "            }}, {{\n",
    "                path: [\"timestamp\"],\n",
    "                operator: Equal,\n",
    "                valueDate: \"{timestamp}\"\n",
    "            }}]\n",
    "        }}) {{\n",
    "          __typename\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result = client.query.raw(query)\n",
    "    print(result)\n",
    "    return len(result['data']['Get']['TextChunk']) > 0  # Returns True if TextChunk exists, False otherwise\n",
    "\n",
    "# text_chunk_exists(client, \"www.bubble.com\", \"2023-10-03T15:30:00+00:00\")\n",
    "already_exists = text_chunk_exists(client, csv_website_address, csv_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "136b511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.batch import Batch \n",
    "from weaviate.util import generate_uuid5 \n",
    "\n",
    "if not already_exists:\n",
    "    # Configure the batch size\n",
    "    client.batch.configure(batch_size=20)\n",
    "\n",
    "    for data in csv_data:\n",
    "        # Split text into chunks using the previously defined function\n",
    "        chunks = split_into_chunks(data[\"text\"], text_chunk_size)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Create each chunk as a separate TextChunk object\n",
    "            text_chunk = {\n",
    "                \"key\": f\"{data['page']}_chunk_{i}\",\n",
    "                \"text\": chunk,\n",
    "                \"pageURL\": data[\"page\"],\n",
    "                \"websiteAddress\": data[\"websiteAddress\"],\n",
    "                \"timestamp\": data[\"timestamp\"]\n",
    "            }\n",
    "\n",
    "            # Generate a unique UUID for this TextChunk\n",
    "            text_chunk_id = generate_uuid5(f\"{data['websiteAddress']}_chunk_{i}{data['timestamp']}\")\n",
    "\n",
    "            # Add the data object to Weaviate using batch\n",
    "            client.batch.add_data_object(data_object=text_chunk, class_name=\"TextChunk\", uuid=text_chunk_id)\n",
    "\n",
    "    # Flush the batch to make sure all data is submitted\n",
    "    client.batch.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f64c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT BATCHING\n",
    "# if not already_exists:\n",
    "#     for data in csv_data:\n",
    "#         # Split text into chunks using the previously defined function\n",
    "#         chunks = split_into_chunks(data[\"text\"], text_chunk_size)\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             # Create each chunk as a separate TextChunk object\n",
    "#             text_chunk = {\n",
    "#                 \"key\": f\"{data['key']}_chunk_{i}\",\n",
    "#                 \"text\": chunk,\n",
    "#                 \"pageURL\": data[\"key\"],\n",
    "#                 \"websiteAddress\": data[\"websiteAddress\"],\n",
    "#                 \"timestamp\": data[\"timestamp\"]\n",
    "#             }\n",
    "\n",
    "#             # Generate a unique UUID for this TextChunk\n",
    "#             text_chunk_id = generate_uuid(f\"{data['websiteAddress']}_chunk_{i}{data['timestamp']}\")\n",
    "\n",
    "#             # Add the data object to Weaviate\n",
    "#             client.data_object.create(data_object=text_chunk, class_name=\"TextChunk\", uuid=text_chunk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "276f0ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [{'class': 'TextChunk',\n",
       "   'description': 'A segmented portion of text from a scraped webpage with full details.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'indexTimestamps': True,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'generative-openai': {'model': 'gpt-3.5-turbo'},\n",
       "    'text2vec-openai': {'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'The identifier for the text chunk.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'key',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'The content of the text chunk.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'text',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'The specific URL of the scraped webpage this chunk belongs to.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'pageURL',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'The address of the website this chunk comes from.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'websiteAddress',\n",
       "     'tokenization': 'whitespace'},\n",
       "    {'dataType': ['date'],\n",
       "     'description': 'The date and time when the chunk was scraped.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': False,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'timestamp'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'trainingLimit': 100000,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba9a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check everything via GraphQL\n",
    "query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    TextChunk {\n",
    "      key\n",
    "      text\n",
    "      pageURL\n",
    "      websiteAddress\n",
    "      timestamp\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "result = client.query.raw(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80de9762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Get': {'TextChunk': [{'text': 'the field’s boundaries in addressing these issues. Techniques such as histogram equalization, gamma correction, SIFT, SURF, RPCA, and the use of CNNs, GNNs, and semi-supervised and unsupervised learning techniques, along with data augmentation strategies, have all been instrumental in overcoming these challenges. Continued investment in research, development, and training of the next generation of computer vision scientists is vital for the field’s evolution. As computer vision advances, it will play an increasingly important role in driving efficiency and innovation in many sectors of the economy and society. Despite the challenges faced, the future of computer vision technology remains promising, with immense potential to reshape our world. Computer vision platforms of tomorrow The most recent wave of generative AI technologies will prove instrumental in shaping the next iterations of computer vision solutions. Today’s computer vision platforms use AI to detect events, objects, and actions that neural networks have been trained to identify, but tomorrow’s platforms may use AI to speculate the outcome of events, objects’ state or positions, and the results of actions before they occur. The true challenge of today’s AI-powered vision-based systems is their narrow understanding. For a model to “know” how to spot more objects, it must be familiar with those things. More knowledge means more training and heavier models. Our society is on the precipice of general AI, which will provide always-on, hyper-intelligent, digital assistants to tomorrow’s enterprises. Such assistants will not just know how to detect things it knows, but they will know how to learn and how to communicate what they see. Replicating human visual understanding has never been closer to a reality as it is today. How can Chooch help you Chooch has radically improved computer vision with generative AI to deliver AI Vision. Chooch combines the power of computer vision with language understanding to deliver more innovative solutions for analyzing video and text data at scale. Whether in the cloud, on premise, or at the edge , Chooch is helping businesses deploy computer vision faster to improve investment time to value. Learn more about Chooch’s generative AI technology, ImageChat™ , which combines the power of computer vision with language understanding to deliver image-to-text capabilities . This remarkable advancement is simply a glimpse into what’s in store for the future. Try it yourself ! Explore its potential and get the free app in the Google Play and App Store . If you are interested in learning how Chooch AI Vision can help you, see how it works and request a demo today. Share Featured Articles Edge AI When is the Right Time to Deploy Edge Computing? AI Definitions What is Computer Vision? ImageChat What is ImageChat? AI Definitions What is Object Detection? Retail 8 Examples of Retail Automation to Future-Proof Your Business Retail 5 AI Use Cases Revolutionizing the Retail Industry Chooch News What is Chooch? Manufacturing 6 Ways Computer Vision is Driving Predictive Maintenance Edge AI Elevating Drone Capabilities: The Sky’s the Limit with Computer Vision AI Definitions 5 Common Problems with ComputerVision and their Solutions Categories AI Definitions Chooch News Cross Industry Deployment Drones Edge AI Enterprise AI Generative AI Geospatial Healthcare ImageChat Machine Learning Manufacturing PPE Product Tutorials ReadyNow Models Remote Monitoring Retail Safety & Security Synthetic Data Telco Uncategorized Visual Data Management Visual Inspection Webinars Related Articles AI Definitions What is Computer Vision? AI Definitions What is Object Detection? AI Definitions A Comparison Guide to Deep Learning vs. Machine Learning AI Definitions The ABCs of Image Annotation for Computer Vision AI Definitions What is an AI Computer? AI Definitions Computer Vision Definitions AI Definitions What is an AI model? AI Definitions What’s the difference between Object Recognition and Image Recognition? Learn more about AI Vision. Reach out to our team today about the benefits of AI Vison from Chooch. See how it works'}, {'text': 'synthetic data can be tailored to create rare events or edge cases that may not be easily encountered in the real world. This helps in training models to handle exceptional scenarios effectively. As the processing power of modern GPUs continues to increase, edge devices can continue to perform complex visual tasks efficiently. Visual processing algorithms benefit from the parallel processing capabilities of GPUs, resulting in faster and more accurate analysis. This enhanced performance is crucial for applications like object detection, facial recognition, and image classification. B usiness benefits of using edge computing with computer vision applications Edge computer vision applications continue to be a catalyst for business growth, fostering productivity, and ensuring safety. It is enabling businesses to make fast er and consistent decisions at scale and positively i mpacting them by: Improving productivity by automating routine visual tasks. Improving workplace safety by ensuring proper PPE usage and alerting of potentially dangerous situations. Improving retail store efficiency from better inventory management, shelf management, and loss prevention. Protecting patient privacy by processing data at the source avoiding transmitting sensitive data to the cloud or central server. Reducing capital equipment downtime through preventative maintenance. Reducing costs by reducing the burden on centralized infrastructure and optimizing bandwidth by extracting data insights locally. Advancing edge computing with Chooch’s edge-optimized computer vision Chooch is at the forefront in leveraging the recent advancements in edge computing and sophisticated model compression algorithms to facilitate successful computer vision model deployments on edge devices. Chooch’s Computer Vision platform delivers embedded vision solutions for video analytics and IoT applications. Chooch helps businesses accelerate edge computer vision adoption by creating easy-to-use software development kits (SDKs), APIs, or programmable or ruggedized edge hardware making it easy for users to build, deploy, and maintain custom computer vision models. Chooch’s computer vision platform comes enabled with Chooch’s pre-trained ReadyNow AI models , including image classification, object detection, facial authentication, action logging, tracking, license plate recognition , and more. They are fully optimized for edge devices; making it easy to deploy computer vision in just minutes. Chooch’s NVIDIA Cloud Validated inference engine can process simultaneous camera feeds and deliver responses in milliseconds with the highest accuracy. Even the smallest edge devices, like the NVIDIA Jetson Nano, can run multiple models with multiple classes in each model using Chooch’s computer vision platform. Depending on the use case, users can configure their edge devices to generate alerts when a detected event occurs or send JSON response outputs to business intelligence and analytics tools to initiate action. Chooch customers are using computer vision technology to automate manual visual review tasks and process video data real-time to gain real-time data insights; enabling them to be proactive in responses rather than reactive. Chooch has helped customers deploy computer vision solutions for workplace safety , retail loss prevention , people counting, object detection, manufacturing QA , wildfire detection, and more; realizing faster time-to-value. Chooch’s AI-powered computer vision solutions help our customers manage their vast amounts of visual data to detect anomalies,understand them, and instantly act. To learn more about Chooch’s edge AI computer vision solutions, contact us to schedule a demo or create a free account to try it yourself . Share Featured Articles Edge AI When is the Right Time to Deploy Edge Computing? AI Definitions What is Computer Vision? ImageChat What is ImageChat? AI Definitions What is Object Detection? Retail 8 Examples of Retail Automation to Future-Proof Your Business Retail 5 AI Use Cases Revolutionizing the Retail Industry Chooch News What is Chooch? Manufacturing 6 Ways Computer Vision is Driving Predictive Maintenance Edge AI Elevating Drone Capabilities: The Sky’s the Limit with Computer Vision AI Definitions 5 Common Problems with Computer Vision and their Solutions Categories AI Definitions Chooch News Cross Industry Deployment Drones Edge AI Enterprise AI Generative AI Geospatial Healthcare ImageChat Machine Learning Manufacturing PPE Product Tutorials ReadyNow Models Remote Monitoring Retail Safety & Security Synthetic Data Telco Uncategorized Visual Data Management Visual Inspection Webinars Related Articles Edge AI Elevating Drone Capabilities: The Sky’s the Limit with Computer Vision Edge AI How to Use AI Computer Vision for Early Wildfire Detection Edge AI The Value of Edge AI — Technologies Advancing Edge AI Adoption Edge AI What is Edge AI? Edge AI Computer Vision and 5G Edge AI Edge AI Edge AI Platform Essentials Learn more about AI Vision. Reach out to our team today about the benefits of AI Vison from Chooch. See how it works'}]}}}\n"
     ]
    }
   ],
   "source": [
    "text_to_search = \"What is computer vision?\"\n",
    "\n",
    "where_filter = {\n",
    "    \"operator\" : \"And\",\n",
    "    \"operands\" : [\n",
    "        {\n",
    "            \"path\": [\"websiteAddress\"],\n",
    "            \"operator\": \"Equal\",\n",
    "            \"valueString\": \"www.chooch.com\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": [\"timestamp\"],\n",
    "            \"operator\": \"Equal\",\n",
    "            \"valueDate\": \"2023-10-03T15:30:00+00:00\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = client.query.get('TextChunk', ['text']) \\\n",
    "    .with_limit(2) \\\n",
    "    .with_near_text({'concepts': [text_to_search]}) \\\n",
    "    .with_where(where_filter) \\\n",
    "    .do()\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
